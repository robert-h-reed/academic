{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01882607-d858-44cd-8458-7a4c0060bf88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Copyright &copy; 2022, 2024, 2025 Scott Jensen, San Jose State University\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">This notebook</span> by <span xmlns:cc=\"http://creativecommons.org/ns#\" property=\"cc:attributionName\">Scott Jensen,Ph.D.</span> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db6c9e82-f1ec-4e85-aa55-034fd1d84c24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Overview\n",
    "This notebook is designed to accompany the notebook titled \"Loading Yelp Data - Serverless\" and should be run ***after*** that notebook has been successfully run. That earlier notebook will have stored the files for the Yelp dataset from Kaggle in a `kaggle` directory in your Databricks catalog. In the Yelp dataset, the review and user data are large JSON files and take a long time to read as DataFrames. This notebook contains code to create tables for the review data (Step 1) and user data (Step 2).\n",
    "\n",
    "Step 0 defines functions used in building both the review and user tables. These functions could also be used to build tables for other datasets.\n",
    "\n",
    "**If you are using this notebook with the Yelp dataset from Kaggle:**\n",
    "* If you left the path settings at their defaults in the `Loading Yelp Data - Serverless` notebook, you can use this notebook as is.\n",
    "* If you modified the paths in the Databricks Catalog where you stored the data, then you will need to update the constants in Steps 1 and 2 accordingly.\n",
    "* In building the table for the user data, it makes the following changes:\n",
    "  * A data error in the elite field is fixed (discussed more below)\n",
    "  * The `friends` field is dropped in building the table\n",
    "\n",
    "**If you are using the notebook with other datasets:**\n",
    "* The two functions used are:\n",
    "  * process_table\n",
    "  * verify_count\n",
    "\n",
    "* The `verify_count` function takes the expected record count as a parameter, so you will need to provide that if building tables for files other than the Yelp data. Alternately, skip using that after the `process_table` function.\n",
    "\n",
    "* The `process_table` function builds the table. Most of the parameters are self-explanatory. The `json_read_function` parameter is the name of a function you define that takes one parameter - the path to the file and then returns a DataFrame for that file. If all the columns and rows in a file are to be included in the table, this could have been included in the `process_table` function itself. If you want to include all of the columns and rows, see the code in Step 1 for building the review table. For the Yelp user data, we need to fix an error in the data and drop a field. If you want to edit the data as part of building the initial table, see the code in Step 2 for an example. The `process_table` function will call the `json_read_function` that was passed as a parameter.\n",
    "\n",
    "**A Note on syntax errors:**\n",
    "\n",
    "You may notice that some of the Spark SQL queries that use an f-string have syntax errors highlighted. These are not actually errors, but seem to be an issue with the serverless version trying to interpret values that won't be set until the query is run. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ba3dc88-d039-42e4-a0e4-318ca273d3cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Building Review and User Tables\n",
    "The review and user data files are the largest Yelp files you will be using in class and in your project.  Loading a DataFrame and selecting the desired columns from larger JSON files is slow, so this notebook instead loads the data and creates Spark Delta tables named `review_table` and `user_table`.  In future sessions, the tables can then be used instead of the JSON files. \n",
    "\n",
    "After the tables are built, if the record count is correct, the original JSON files can be deleted.\n",
    "\n",
    "<span style=\"color:red;\">Be sure to run this notebook after first running the notebook that loads the data files into a volume in the Databricks Catalog as bzip2 files.</span>\n",
    "\n",
    "The first time this notebook is run, it will take up to 10 minutes to build both tables from the JSON files (this can vary a bit).\n",
    "\n",
    "As noted above, once the tables are built, they can be used in Spark SQL queries the same as temporary views in SQL queries.\n",
    "\n",
    "###Step 0: Function Definitions\n",
    "The following cell includes imports and function definitions that are needed when running the cells further down for building the `review_table` and `user_table`, so cells should be run in order from top-to-bottom or using the `Run all` button or menu option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2326cb8c-de82-4a07-b6c1-f5c6eb6b31f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "\n",
    "def file_exists(path):\n",
    "  \"\"\"Checks if the path provided is a file.\n",
    "\n",
    "    Parameters:\n",
    "     path (str): Path to the directory in the Databricks catalog\n",
    "\n",
    "     Returns True if the path exists and contains files \n",
    "     otherwise False\n",
    "  \"\"\"\n",
    "  try:\n",
    "    file_list = dbutils.fs.ls(path)\n",
    "    return len(file_list) > 0\n",
    "  except Exception:\n",
    "    print(f'The specified file ({path}) does not exist or the path is not valid.')\n",
    "    return(False) #if path does not exist\n",
    "\n",
    "\n",
    "def process_table(table_name, table_path, data_path, force_rebuild, json_read_function):\n",
    "  '''Creates a table based on the JSON file (can be compressed using bzip2).\n",
    "  \n",
    "  Parameters:\n",
    "     table_name (str): Name of the table being created\n",
    "     table_path (str): Dot-separated path to the catalog and schema - generally workspace.default\n",
    "     data_path (str): Volume path of the compressed JSON data file\n",
    "     force_rebuild (bool): Flag as to whether the table should be rebuilt (True) from the JSON files even if the table exists\n",
    "     json_read_function (function): Name of a function that takes one parameter, the path to the JSON data, and returns the DataFrame that was created\n",
    "     Returns None\n",
    "     Raises an exception if the path to the data file is not valid\n",
    "  '''\n",
    "  spark.sql(f'USE {table_path}')\n",
    "  if ( spark.catalog.tableExists(table_name) and force_rebuild == False):\n",
    "    print(f\"{table_name} table already exists and the option to force the rebuild was false\")\n",
    "    return\n",
    "  \n",
    "  data_exists = file_exists(data_path)\n",
    "  if force_rebuild:\n",
    "    if not data_exists:\n",
    "      raise Exception(f\"The path to the compressed JSON data file: {data_path} does not exist. Any existing table was not deleted.\")\n",
    "    spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "  elif not data_exists:\n",
    "    raise Exception(f\"The path to the compressed JSON data file: {data_path} does not exist. Be sure you loaded the data to the correct catalog schema.\")\n",
    "\n",
    "  print(f\"building {table_name} from JSON file\")\n",
    "  df_temp = json_read_function(data_path)\n",
    "  # Does not already exist as a table, so write it out\n",
    "  df_temp.write.mode(\"overwrite\").saveAsTable(f'{table_path}.{table_name}')\n",
    "  return\n",
    "  \n",
    "\n",
    "def verify_count(table_name, table_path, target_count):\n",
    "  \"\"\"Verifies that the table has the expected number of records.\n",
    "\n",
    "  Parameters:\n",
    "     table_name (str): Name of the table that was created\n",
    "     table_path (str): Dot-separated path to the catalog and schema - generally workspace.default\n",
    "     target_count (int): Number of records expected in the table\n",
    "     Returns None\n",
    "  \"\"\"\n",
    "  table = table_path + '.' + table_name\n",
    "  result = spark.sql(f\"\"\"\n",
    "  SELECT COUNT(*) AS record_count\n",
    "  FROM {table}\n",
    "  \"\"\")\n",
    "  # result is a DataFrame with one row and column (for the COUNT)\n",
    "  # The collect() method returns an array of Spark DataFrame Rows,\n",
    "  # So in the following line we:\n",
    "  # 1. Get the first row from the array returned by collect\n",
    "  # 2. Convert the array to a list\n",
    "  # 3. Get the first element in the list (the int with the count)\n",
    "  record_count = list(result.collect()[0])[0]\n",
    "\n",
    "  if record_count == target_count:\n",
    "    print(f\"Table {table_name} was created and had the expected record count of {target_count:,d}. \"\n",
    "           \"The data file can be deleted.\")\n",
    "  else:\n",
    "    print(f\"Table {table_name} was created, \"\n",
    "          f\"but instead of the expected record count of {target_count:,d}, \"\n",
    "          f\"it had a record count of {record_count:,d}. \"\n",
    "          f\"The data file should NOT be deleted \"\n",
    "          \"and this needs to be debugged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e53234-0b22-4714-8e7a-16a543db797d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 1: Build the table for the review data\n",
    "The following cell uses the functions defined in Step 0 above, so that must be run first.  \n",
    "\n",
    "For the review data, we want all the columns and rows from the review data file included in the table, so the function passed as a parameter to the `process_table` function simply reads the file and returns a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ffe5672-ea7f-42ac-8986-92ebf7717013",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If you want to force the building of the table from the JSON files, set FORCE_REVIEW_REBUILD to True.\n",
    "# Only do this if records are missing from the table, it's not able to create the table, or you want to change the fields \n",
    "# included in the table since it is time consuming.\n",
    "# ************************************************************************************************\n",
    "# AFTER RUNNING IT TO REBUILD FROM THE JSON DATA BE SURE THAT FORCE_REVIEW_REBUILD IS SET TO FALSE\n",
    "# ************************************************************************************************\n",
    "FORCE_REVIEW_REBUILD = False \n",
    "REVIEW_DATA_PATH = \"/Volumes/workspace/default/kaggle/review.bz2\"\n",
    "REVIEW_TBL_PATH = \"workspace.default\"\n",
    "REVIEW_TBL_NAME = \"review_table\"\n",
    "REVIEW_TARGET = 6990280\n",
    "\n",
    "# If any fields should be added or excluded, such transformations should be included in this function.\n",
    "# The function defined must take one parameter - the path to the JSON data file.\n",
    "def create_review_dataframe(data_path):\n",
    "  df_reviews = spark.read.json(data_path)\n",
    "  return(df_reviews)\n",
    "\n",
    "process_table(REVIEW_TBL_NAME, REVIEW_TBL_PATH, REVIEW_DATA_PATH, FORCE_REVIEW_REBUILD, create_review_dataframe)\n",
    "verify_count(REVIEW_TBL_NAME, REVIEW_TBL_PATH, REVIEW_TARGET)\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {REVIEW_TBL_PATH}.{REVIEW_TBL_NAME} LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d03e776-db5e-4349-a068-f0cfc4d1ecc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 2: Build the table for the user data\n",
    "The following cell uses the functions defined in Step 0 above, so that must be run first.  \n",
    "\n",
    "**NOTE ON TRANSFORMATIONS IN LOADING:** \n",
    "\n",
    "The `process_table` function takes the name of a function that will be used to load the data.  That function takes one parameter - the name of the file used to create the table. Any transformations you want to make to the raw data can be done in that function.\n",
    "\n",
    "Due to the way Yelp created the academic dataset, we need to fix a data error (this is not uncommon in working with data). It appears the data on years a user is elite might be stored as a string of 4-digit years\n",
    "such as \"2017201820202021\".  Since Yelp did not exist during the prior century (and it's over 70 years until the next century), someone may have decided to split that string into an array (like a list in Python) based on \"20\". This would work for every year this century except for 2020, which unfortunately is within our data range, so the JSON for the user with the elite string \"2017201820202021\" is already in the JSON user file as [2017,2018,20,20,2021], so we need to fix that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffd4952-e844-431f-ad94-fea9520fa89f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#If you want to force the building of the table from the JSON files, set FORCE_USER_REBUILD to True.\n",
    "# Only do this if records are missing from the table, it's not able to create the table, or you want to change the fields \n",
    "# included in the table since it is time consuming.\n",
    "# **********************************************************************************************\n",
    "# AFTER RUNNING IT TO REBUILD FROM THE JSON DATA BE SURE THAT FORCE_USER_REBUILD IS SET TO FALSE\n",
    "# **********************************************************************************************\n",
    "FORCE_USER_REBUILD = False \n",
    "USER_DATA_PATH = \"/Volumes/workspace/default/kaggle/user.bz2\"\n",
    "USER_TBL_PATH = \"workspace.default\"\n",
    "USER_TBL_NAME = \"user_table\"\n",
    "USER_TARGET = 1987897\n",
    "\n",
    "# If any fields should be added or excluded, such transformations should be included in this function.\n",
    "# Here we are fixing a data error in the elite field and dropping the \"friends\" field. The friends field \n",
    "# contains a list of the user IDs for everyone on Yelp that has friended this user (regardless of whether that\n",
    "# other user is in the dataset). For some popular users, this can run into the thousands. \n",
    "def create_user_dataframe(data_path):\n",
    "  df_users = spark.read.json(data_path).\\\n",
    "  withColumnRenamed(\"elite\",\"original_elite\").\\\n",
    "  withColumn(\"elite\", f.regexp_replace( f.col(\"original_elite\"), '20,20','2020').alias(\"elite\") ).\\\n",
    "  drop(\"original_elite\").drop(\"friends\")\n",
    "  return(df_users)\n",
    "  \n",
    "process_table(USER_TBL_NAME, USER_TBL_PATH, USER_DATA_PATH, FORCE_USER_REBUILD, create_user_dataframe)\n",
    "verify_count(USER_TBL_NAME, USER_TBL_PATH, USER_TARGET)\n",
    "\n",
    "spark.sql(f\"SELECT * FROM {USER_TBL_PATH}.{USER_TBL_NAME} LIMIT 5\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9ab2081-41e6-43ad-ae3a-0369c9718242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Step 3: Are all of the files and tables present?\n",
    "\n",
    "**This step is used for grading the assignment**\n",
    "\n",
    "For most of the exercises, grading will be based on multiple cells, and while you will know the output you are looking for, you will not know which cells are graded or how they are weighted in the grading beforehand. The goal in this exercise is to make sure you have all the data  loaded so you can participate in the exercises and your team project.\n",
    "\n",
    "The code in the following two cells lists the contents of your `/kaggle` directory in the Databricks Catalog and lists the tables you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2666b4e-0247-43c7-8cb4-fb2c8d64ffab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls('/Volumes/workspace/default/kaggle/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37db4c2e-03c2-4543-96f9-87575b2c35be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {REVIEW_TBL_PATH}.{REVIEW_TBL_NAME} LIMIT 5\").show()\n",
    "spark.sql(f\"DESCRIBE TABLE {REVIEW_TBL_PATH}.{REVIEW_TBL_NAME}\").show()\n",
    "spark.sql(f\"SELECT * FROM {USER_TBL_PATH}.{USER_TBL_NAME} LIMIT 5\").show()\n",
    "spark.sql(f\"DESCRIBE TABLE {USER_TBL_PATH}.{USER_TBL_NAME}\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Building Review and User Tables - Serverless",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}